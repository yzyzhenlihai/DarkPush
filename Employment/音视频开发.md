音视频学习路线

![img](images/picture1.png)





## 音视频入门知识

###  图像

#### YUV

Y表示亮度，U/V表示色度

**YUV采样格式** 

主流的采样格式有YUV444、YUV422、YUV420。

YUV444：每1个Y对应一组UV分量，每个像素的三个分量都是 8 bit，也就是1个像素需要3个字节。

YUV420：每2个Y对应一组UV分量，UV 分量是 Y 分量采样的一半，Y 分量和 UV 分量按照 2 : 1 的比例采样。

![](./images/picture8.png)

YUV422：每4个Y对应一组UV分量。

<img src="./images/picture7.png" style="zoom:67%;" />

`yuyv422` 是许多摄像头支持的**原始像素格式**，适合高效捕获和处理



**YUV422转化为YUV420**

其中`y u v` 分别都是1个字节的大小，对于YUV422，数据的组织格式是`yu yv yu yv` ，这里`yu` 是一个像素，大小为两字节。

- **Y 分量（亮度）**：
  - YUYV422 和 YUV420P 的 Y 分量采样比例都是 1:1（每个像素一个 Y 值）。
  - 因此，Y 分量保持不变，直接从 YUYV422（格式：`Y0 U0 Y1 V0`）的 `Y0`、`Y1` 逐字节复制到 YUV420P 的 Y 平面（`frame->data[0]`），不丢弃任何数据。
  - 例如，640×480 分辨率，Y 分量大小为 640×480 = 307,200 字节。
- **U 和 V 分量（色度）**：
  - YUYV422：采用 4:2:2 采样，每 2 个像素（水平方向）共享一组 U、V 分量。
    - 数据格式：`Y0 U0 Y1 V0`，每 4 字节表示 2 个像素，U、V 各占 1 字节。
    - 例如，640×480 分辨率，U 和 V 各有 640/2 × 480 = 153,600 个值。
  - YUV420P：采用 4:2:0 采样，每 4 个像素（2×2 块，水平和垂直各 2:1 下采样）共享一组 U、V 分量。
    - U 平面和 V 平面大小为 `(width/2) × (height/2)`。
    - 例如，640×480 分辨率，U 和 V 各有 640/2 × 480/2 = 320×240 = 76,800 个值。
  - 下采样过程：
    - 水平方向：每 2 个像素取 1 个 U、V（从 `Y0 U0 Y1 V0` 中取 U0、V0）。
    - 垂直方向：每 2 行取 1 行 U、V（**通常取偶数行，丢弃奇数行数据**）。
    - 结果：U、V 分量数据量从 YUYV422 的 153,600 减少到 YUV420P 的 76,800，丢弃了一半的色度数据。这种丢弃是正常的，因为 4:2:0 格式通过降低色度分辨率来减少数据量，而人眼对色度变化的敏感度较低，视觉影响较小。

**时间戳设置** 

`time_base` 是 FFmpeg 中用于表示时间单位的结构体（`AVRational`），由分子（`num`）和分母（`den`）组成，表示时间刻度的倒数（秒/刻度）

`AVFrame` 的 `pts` 表示帧的呈现时间，单位是 `AVCodecContext` 的 `time_base`。修改 `frame->pts` 时，必须确保其值与编码器的 `time_base` 匹配。`pts * time_base` 就可以获得某一帧所在的时间。

知道帧率`fps` ，每秒帧的数量，就可以知道每帧所在的时间`1/fps`

**frame字节对齐**

`frame->linesize` 是 `AVFrame` 结构体中的一个数组，用于表示帧中每个平面（plane）的**每行数据的字节大小**

由于内存对齐（例如，某些硬件要求行数据按 16 字节或 32 字节对齐），`linesize` 可能大于实际像素数据所需的字节数。

它用于正确索引帧数据（如 `frame->data[0][i * frame->linesize[0]]` 访问 Y 平面第 i 行的起始地址）

`av_frame_get_buffer(frame, 32); // 分配缓冲区，按 32 字节对齐`

**YUV存储格式** 

◆ packet：打包格式，即先存储一个yuv，再存储下一个yuv；

◆ planar：平面格式，即先存储y平面，再存储u平面，再存储v平面；

◆ semi-planar：先存储y平面，再存储uv平面；

**VLC无法直接打开YUV文件**

- YUV 文件是原始视频数据，通常不包含元信息（分辨率、帧率、像素格式）。
- VLC 需要通过命令行参数（如 `--rawvid-width`、`--rawvid-fps`）显式指定这些信息，否则会报错（出现之前遇到的 “Picture size 0x0 is invalid” 和 “invalid or no framerate specified”）。
- `vlc --demux=rawvid --rawvid-fps=30 --rawvid-width=640 --rawvid-height=480 --rawvid-chroma=I420 ./video.yuv` 指定解封装器、分辨率、帧率、像素格式。



#### H.264

H.264 是一种视频压缩编码标准

**视频编码层（Video Coding Layer, VCL）**
视频编码层负责将原始视频数据进行编码，它定义了如何将原始视频数据压缩成更小的格式，VCL使用多种编码工具和技术来实现高效的压缩。

**网络抽象层（Network Abstraction Layer, NAL)**
网络抽象层负责将视频编码层产生的比特流组织成适合传输和存储的格式。它的主要任务是将编码数据打包成网络抽象层单元（Network Abstraction Layer Unit，NALU），这些单元可以在不同的网络和存储介质上进行传输和存储。

简单的说，视频编码层将原始视频数据压缩编码，网络抽象层将压缩编码后的数据打包成NALU（网络抽象层单元）来进行传输或存储。



### 音频

#### PCM

PCM(Pulse Code Modulation，脉冲编码调制)音频数据是未经压缩的音频采样数据**裸流**，它是由模拟信号经过采样、量化、编码转换成的标准数字音频数据。

**描述PCM数据的6个参数：**

* Sample Rate : 采样频率。8kHz(电话)、44.1kHz(CD)、48kHz(DVD)。

* Sample Size : 量化位数。常见值为8-bit、16-bit。

*  Number of Channels : 通道个数。常见的音频有立体声(stereo)和单声道(mono)两种类型，立体声包含左声道和右声道。另外还有环绕立体声等其它不太常用的类型。

* Sign : 表示样本数据是否是有符号位，比如用一字节表示的样本数据，有符号的话表示范围为-128 ~ 127，无符号是0 ~ 255。

* Byte Ordering : 字节序。字节序是little-endian（小端）还是big-endian（大端）。通常均为little-endian。

* Integer Or Floating Point : 整形或浮点型。大多数格式的PCM样本数据使用整形表示，而在一些对精度要求高的应用方面，使用浮点类型表示PCM样本数据。



#### AAC

AAC(Advanced Audio Coding，高级音频编码)是一种声音数据的文件压缩格式。AAC分为ADIF和ADTS两种文件存储格式。

* **ADIF**：Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在**磁盘文件**中。

* **ADTS**：Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始，适合**流媒体**。



### 字幕

#### SSA

SSA（SubStation Alpha），是由CS Low（亦称Kotus）创建，比传统字幕格式（如SRT）功能更加先进的字幕文件格式。

该格式字幕的外挂文件以***.ssa**作为后缀。

#### ASS

ASS（Advanced SubStation Alpha），是一种比SSA更为高级的字幕格式, 其实质版本是SSA v4.00+，它是基于SSA 4.00+编码构建的。

ASS的主要变化就是在SSA编写风格的基础上增添更多的特效和指令。

该格式字幕的外挂文件以***.ass**作为后缀。

#### SSA/ASS基本结构

**ini风格：**通过分段（Sections）和键值对（Key-Value Pairs）组织信息

SSA/ASS字幕是一种类ini风格纯文本文件；包含五个section：[Script Info]、[v4+ Styles]、[Events]、[Fonts]、[Graphics]。

* [Script Info]：包含了脚本的头部和总体信息。[Script Info] 必须是 v4 版本脚本的第一行。

* [v4 Styles]：包含了所有样式的定义。每一个被脚本使用的样式都应该在这里定义。ASS 使用 [v4+ Styles]。

* [Events]：包含了所有脚本的事件，有字幕、注释、图片、声音、影像和命令。基本上，所有在屏幕上看到的内容都在这一部分。

  **真正的字幕内容**，规定字幕的出现时间、消失时间、文字内容以及是否包含特效

* [Fonts]：包含了脚本中内嵌字体的信息。

* [Graphics]：包含了脚本中内嵌图片的信息。





### 视频封装格式     



#### FLV 



#### TS

根据TS header中的pid字段，可以确定负载是PAT表、PMT表（指明音视频流的PID值）、视频流、音频流。

PAT表的功能就是找PMT表的PID

利用PMT表就能找到音频和视频的PID



### 协议

- RTP(Real-time Transport Protocol)，实时传输协议。

- RTCP(Real-time Transport Control Protocol)，实时传输控制协议。

- RTSP(Real Time Streaming Protocol)，实时流协议。

-  RTMP(Real Time Messaging Protocol)，实时消息传输协议。

- HLS(HTTP Live Streaming)，苹果公司提出的基于HTTP的流媒体网络传输协议。

- HTTP-FLV，将RTMP等负载信息携带在HTTP协议之上的码流传输协议。



### FFmpeg库

音频数据的重采样?



#### FFmpeg命令行

**`split` 滤镜**：将视频流拆分为多个子流，便于并行处理。

**`crop` 滤镜**：从输入视频中裁剪指定区域。

**`vflip` 滤镜**：对视频进行垂直翻转。

**`overlay` 滤镜**：将一个视频叠加到另一个视频上，可以通过 `x` 和 `y` 参数指定叠加位置。

#### FFmpeg编译

`./configure --prefix=$(pwd)/build`

`--prefix` 指定ffmpeg安装位置，默认位置安装在/usr/local目录下

`--enable-static|--enable-shared` 选择编译成静态库还是动态库，只能选一个   

`--enable-gpl` 和`--enable-libx264` 一般同时启用

FFmpeg 的 RTSP 支持是内置在 `libavformat` 中，并通过 `--enable-network` 自动启用，而无需显式指定 `--enable-protocol=rtsp`



#### FFmpeg头文件

##### libavformat

**AVFormatContext**

是 FFmpeg 中用于管理输入/输出格式的结构体，保存了视频设备的上下文信息（如输入格式、流信息等）。

**AVDictionary**

 是FFmpeg 中用于传递键值对配置参数的结构体。



##### libavcodec

**avcodec_register_all**

函数在 FFmpeg 的较新版本中已经被弃用（deprecated），从 FFmpeg 4.0 开始，FFmpeg 引入了新的模块化设计，移除了对 `avcodec_register_all()` 的强制依赖。编解码器的注册现在是按需加载 或 自动完成，不再需要显式调用此函数。调用 `avcodec_find_decoder()`、`avcodec_find_encoder()` 或其他相关函数时，FFmpeg 会自动初始化和注册所需的编解码器。



##### libavutil

**AVFrame**

`av_frame_alloc()` 为frame分配空间，结构体本身的内存，但是不分配数据缓冲区（frame->data和frame->buf指向的内存）。

`av_frame_get_buffer` 为frame分配数据缓冲区，根据不同的YUV格式计算的。





### SDL库



### pacman命令

1. **查询 云端仓库是否有某个软件**

```
pacman -Ss sdl
```

`-Ss` 后面是 正则表达式字符串

2. **查看云端软件的具体信息**

```
pacman -Si mingw-w64-x86_64-SDL2
```

3. **安装一个软件**

```
pacman -S mingw-w64-x86_64-SDL2
```

4. **查询一个软件是否安装**

```
pacman -Qs mingw-w64-x86_64-SDL2
```

5. **查询本地已安装软件的具体信息**

```
pacman -Qi mingw-w64-x86_64-SDL2
```

6. **查询本地已安装软件包所包含文件的列表**

```
pacman -Ql mingw-w64-x86_64-SDL2
```

7. **删除一个软件**

```
pacman -R mingw-w64-x86_64-SDL2
```

8. **查看某个包是否缺少文件**

```
pacman -Qk mingw-w64-x86_64-SDL2
```



## Linux多用户流媒体并发服务器开发

**设计思路**

1. 对于客户端是否关闭，服务端很难发觉，一般会让客户端定时发送心跳包给服务端。但是在之前的实现中，服务端都是send或recv函数判断客户端是否关闭的，当返回值ret==0表示客户端以及关闭了，但是这种方式不太准确，因为客户端可能并没有调用close，而是由于网络过于拥塞。（**心跳包很重要**）
2. 对于ffmpeg中一些需要用指定API才能释放空间的结构体或对象，如`AVFormatContext、AVCodecContext、AVFrame、AVPacket` ，有以下两种方式管理内存：
   - 利用智能指针管理内存空间，需要自定义删除器。
   - 利用RAII机制管理内存空间，相当于是智能指针的复杂实现。RAII 通过将资源的生命周期绑定到对象的生命周期，确保资源在对象析构时自动释放，非常适合 FFmpeg 结构体的管理
3. 单个服务器视频转发设计
   - 每个客户端连接启用一个发送线程来发送数据包。
   - 每条客户端连接配置一个缓存队列，负责缓存需要发送的数据包。
4. 大规模可伸缩架构
   - 主从架构，主服务器负责管理从服务器（视频服务器），分配从服务器给请求的客户端。
   - 从服务器定期发送自身状态给主服务器。
5. 高性能服务器
   - 写高性能流媒体服务器，UDP主要用来做音视频传输。就意味着我们要自己实现一套UDP丢包重传，拥塞控制，分包，组包这些操作
   - 写视频服务器，TCP并不是没有用，利用TCP可靠性的特点。我们利用TCP来传命令。
   - 结合TCP+UDP各自的优劣来做流媒体开发。
6. 在服务器发送数据时，都会先发送包头（包头一般会包括数据的类型，数据的大小），再发送数据（客户端根据包头的元数据，再读取指定大小的数据）

**音视频流媒体开发流程**

<img src="./images/picture9.png" style="zoom:67%;" />

`cheese -d /dev/video0` 	Ubuntu下需要指定视频文件，直接使用`cheese`会报错

## 就业相关



#### 音视频就业方向

1. 视频剪辑SDK。
1. 学习Qt OpenGL的同学，工作中专注做OpenGL方向开发， 是有必要学习一下游戏引擎的。 UE Unity3d cocos2dx(H5)本身不太好选择。 如果想做移动手机，AR方向，unity3d是最好的选择。 做PC 3A游戏开发选择UE。 Godot开源免费，可以看作开源版本的Unity3d，类似windows vs linux。也是非常友好的。如果你想深入游戏引擎源代码是最好的选择。 另外国内cocos2dx 在手机小游戏方向也是非常优秀的。3D领域如果你要把全流程打通，需要学习的软件非常多。雕刻建模材质贴图骨骼动画后期 blender/zbrush substancepainter GIMP 都要学习，至少要知道整个流程如何跑的。

### 音视频招聘企业

万兴科技、影石、声网
